Our algorithm evaluation confirms that DiffTrack can robustly trace changes on code pieces.
We thus conducted a quantitative study to investigate how well the CodeGlass interface can support comprehension of code pieces. 

\begin{table*}[t]
    \centering
    \begin{tabular}{c||ccc|ccc}
    \multirow{2}{*}{Category} & \multicolumn{3}{c|}{\textbf{without CodeGlass}} & \multicolumn{3}{c}{\textbf{with CodeGlass}} \\
     & Precision & Recall & Confidence & Precision & Recall & Confidence \\ \hline \hline
     Execution & 0.77 (0.12) & 0.46 (0.08) & 74.5 (9.67) & 0.80 (0.18) & 0.49 (0.13) & 79.5 (7.14) \\
     Rationale & 0.54 (0.38) & 0.14 (0.09) & 63.9 (28.1) & 0.79 (0.21) & 0.29 (0.06) & 78.3 (11.6) \\
     History & 0.35 (0.35) & 0.08 (0.02) & 8.6 (17.7) & 0.88 (0.13) & 0.16 (0.01) & 66.6 (6.25) \\ 
    \end{tabular}
    \caption{Accuracy performance and confidence scores in the user evaluation. Values in parentheses represent the standard deviations.}
    \label{table:experiment_result}
    \vspace{-2mm}
\end{table*}

\begin{table}[t]
    \centering
    \begin{tabular}{rl||ccc}
    \multicolumn{2}{l||}{Metric} & $t(7)$ & $p$ & Cohen's $d$ and 95\%CI \\ \hline \hline
    \multicolumn{2}{l||}{\textbf{Execution}} & & & \\
    & Precision & 0.76 & \textit{n.s.}%0.47
    & 0.27 [-0.44, 0.97] \\
    & Recall & 0.74 & \textit{n.s.}%0.48 
    & 0.26 [-0.45, 0.96] \\
    & Confidence & 1.77 & \textit{n.s.}%0.12 
    & 0.63 [-0.16, 1.37] \\ \hline
    \multicolumn{2}{l||}{\textbf{Rationale}} & & & \\
    & Precision & 1.79 & \textit{n.s.}%0.12 
    & 0.63 [-0.15, 1.38] \\
    & Recall & 3.55 & < 0.01 & 1.25 [0.28, 2.18] \\
    & Confidence & 1.57 & \textit{n.s.}%0.16 
    & 0.55 [-0.21, 1.29]\\ \hline
    \multicolumn{2}{l||}{\textbf{History}} & & & \\
    & Precision & 4.58 & < 0.01 & 1.62 [0.52, 2.68] \\
    & Recall & 5.97 & < 0.001 & 2.11 [0.81, 3.38] \\
    & Confidence & 5.41 & < 0.001 & 1.91 [0.69, 3.09] \\ 
    \end{tabular}
    \caption{Paired t tests on user evaluation performance results.}
    \label{table:stats_result}
    \vspace{-2mm}
\end{table}


\subsection{Experimental Design}

\subsubsection{Hypothesis}
Our literature survey identified the following five information categories developers seek in code comprehension:

\begin{itemize}
\setlength{\itemsep}{0cm}
\item \textbf{Execution}: What the given code piece executes~\cite{Information_Needs_in_Collocated_Software_Development_Teams}.
\item \textbf{Rationale}: Why the given code piece is implemented in the way it is~\cite{Information_Needs_in_Collocated_Software_Development_Teams}.
\item \textbf{History}: How the given code piece has been evolved~\cite{Chronicler}.
\item \textbf{Contributor}: Who was involved in the implementation of the given code piece~\cite{Who_can_help_me_with_this_source_code_change}.
\item \textbf{Usage}: Where and how the given code piece is used in a project~\cite{Six_Learning_Barriers_in_End_User_Programming_Systems}.
\end{itemize}

We found that existing tools support Contributor and Usage information seeking (e.g., git-blame to identify who made a particular change).
Therefore, we decided to mainly examine the user performance of finding information for \textbf{Execution}, \textbf{Rationale}, and \textbf{History} in our evaluation.
These information categories were also found important by the professional developers in our interview study.

% We then developed the following hypotheses to derive our experimental design:

% \begin{itemize}
% \setlength{\leftskip}{4mm}
% \setlength{\itemsep}{0mm}
% \item[H1.] \textit{CodeGlass would not degrade accurate and precise execution understanding for code pieces.} This is because that CodeGlass does not prevent participants from examining raw code.  
% \item[H2.] \textit{CodeGlass would support more accurate and precise rationale understanding for code pieces.} This is because that past pull requests in CodeGlass can contain information about why code changes were made.  
% \item[H3.] \textit{CodeGlass would support more accurate and precise understanding of development history for code pieces.} This is because that a series of relevant past pull requests in CodeGlass can suggest the evolution of code pieces.
% \end{itemize}



\subsubsection{Task}
Debugging is a common task to assess the effect of code comprehension tools \cite{Code_Bubbles,Improving_API_Documentation_Usability_with_Knowledge_Pushing}.
In such a task, participants are asked to identify and fix intentionally-introduced bugs.
However, our system involves version control, and thus participants would be able to easily identify what changes would be needed to complete a task.



Instead, we decided to employ short documentation creation tasks.
We provided a simple text format for documenting execution, rationale, and history information.
Participants were asked to itemize any important information in these categories about the given code piece.
For each item, they also provided their confidence score from 0 to 100 in order to indicate how confident they were that the description was correct.
A higher score means a description with stronger confidence.
To avoid making our experiment unnecessarily long, we set the time limit for each task to 20 minutes.

We created four tasks using the repository of Chart.js in this experiment.
Two of the authors jointly examined the repository and developed information items that both agreed were important enough to be documented.
The tasks had 10.8, 9.8, and 7.3 items on average as answer keys for execution, rationale, and history categories, respectively.
Each task used a code piece from a different source code in Chart.js to avoid learning effects.
We chose the following files under the same directory: core.animation.js, core.canvasHelpers.js, core.ticks.js, and core.title.js\footnote{Note that this was moved to plugin.title.js as of March 26, 2017.}. 


\subsubsection{Procedure}
The participants were asked to come to our laboratory for this study.
After they signed a consent form, we explained the CodeGlass interface and tasks.
When the participants felt comfortable with the tasks and CodeGlass, we started the experiment.
We had four tasks in total (Task A, B, C, and D) and two conditions: the GitHub Web interface with the presence and absence of CodeGlass (CG and non-CG).
The two sets of tasks were counter-balanced for the two interface conditions across participants.
We fixed the order of the tasks in each set.
We alternatively switched the interface conditions with always starting with the reference condition.
Thus, the order for the half of our participants was Task A/non-CG; Task B/C
G; Task C/non-CG; and Task D/CG. 
The rest of the participants were exposed to Task B/non-CG first, followed by Task A/CG; Task D/non-CG; and Task C/CG. 
As all of our participants had sufficient experience on GitHub, we expected that starting with the non-CG condition would not cause undesirable learning effects.

After the participants completed all four tasks, we conducted a short semi-structured interview to understand their subjective impressions about CodeGlass and its potential use.
The participants were offered a compensation of approximately 15 USD cash in a local currency.

\subsubsection{Participants}

As our tasks involve code comprehension, we set several criteria for participants: 1) their programming experience should be longer than one year; 2) they should be familiar with GitHub; 3) they should be knowledgeable in JavaScript; and 4) they were not involved in the Chart.js project.
Our participants would thus represent developers who have enough knowledge and experience on reading code but do not own the background about a project.
Our selection criteria also reflected one of our target use cases of CodeGlass: junior developers who join a new team now need to acquire understanding of source code in a project to engage in various tasks.
We mainly recruited participants from our institute.
As a result, 8 people volunteered (PC1--8; all male; 22.1 years old on average).

 

\subsection{Results}


%\subsubsection{Quantitative Results}
We counted how many points in our answer keys documentation created by our participants covered.
We then calculated the precision and recall based on the number of correct items similar to the evaluations above.
When there was no documented item for a particular category, we regarded all evaluation metrics as 0.
Table~\ref{table:experiment_result} shows the means and standard deviations of the precision, recall, and confidence scores.
The precision values in the CodeGlass condition were high for all documentation categories.
The recall values for rationale and history documentation had improvements with CodeGlass.
Table~\ref{table:stats_result} summarizes results of paired t tests.
We found significant differences for recall and all scores in rationale and history documentation, respectively.

%\subsubsection{Subjective feedback}
During the post-experimental interviews, our participants expressed various potential use of CodeGlass: understanding unfamiliar code (7 participants); learning how to fix bugs (3 participants); and self-training with open source projects (2 participants).
Four participants explicitly mentioned that the response speed of CodeGlass was fast enough for their interaction.

% \subsubsection{Comments from Participants}
% % Four of our participants (P2, P3, P6, and P7) explicitly mentioned that the response speed of CodeGlass was fast enough for their interaction.
% % The rest of the participants did not complain its responsiveness, and we thus concluded that CodeGlass was well implemented for practical use.

% Participants mentioned several interesting use scenarios of CodeGlass.
% P5 commented that CodeGlass could help him understand part of code which he is not familiar with.
% This is in line with our main target use scenario informed through interviews with professional developers.

 % %P5: 自分が使いたいと思うときっていうのはやっぱり複数人でやってるんだったら自分が担当してない部分のコードを理解したいなーって思うときにこれを。
% \myquote{I want to use this (CodeGlass) when I work in a team and want to understand a part of code which I am not in charge of.}{PB5}

% PB3 shared his view with us that development history information provided by CodeGlass could inform how to fix and even avoid possible bugs.
% %P3: むしろこれはなんか、プログラム中級者がさ、開発歴見てああ俺もこういうバグやっちまうわーみたいなので共感して、ああこう治すんだっていう知識を仕入れるときに良さそう。
% \myquote{I think this (CodeGlass) is useful when intermediate-level programmers can look at development history and see `Ah, I would do the same mistake.' Then they can learn how to fix such bugs.}{PB3}

% We also asked our participants how they would use CodeGlass for open source projects.
% Two participants (PB5 and PB7) responded that CodeGlass can help self-learning.
% PB7 commented that he could exploit open source projects much more than the default GitHub interface for his self-learning.
% %P5: オープンソースでやるなら、そうだな自分も全然ただの開発に携われるようなスキルを持ってるわけじゃないから直接連絡とかは取れないしって思うんだったら全然これを使って勉強するっていうか、理解するのはやっぱり無いよりかは分かりやすかったと僕は思いました。}
% %\myquote{Well, I do not have enough skills to get involved in (open source) projects. So I am hesitant to directly contacting contributors. But if I can use this (CodeGlass), I can learn code by myself.}{PB5}
% %P7: だから、どんどん新しいそのなんでも教材に出来るっていうか。どんなそのGitHubのものも、例えば先生を見つけてくれば、簡単なものから複雑なものまで、全部どういうコードでこの人が書いたかっていうのが、今まではソースとかコメントしか読めなかったけども、プロジェクト自体がどうやって運んでいくかっていうのもちょっと引いた目で見るみたいなことがいろいろな教材も無限にあるし、っていうのでいいんじゃないかなと。
% \myquote{I can use anything as a learning material. If I can find a good example, regardless of simple or complex code, I can see how developers wrote code. Until now, I can only read source code and comments. But [with CodeGlass] I can have a higher-level view on how the project is going on.}{PB7}
